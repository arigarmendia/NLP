![](https://github.com/arigarmendia/NLP/blob/main/banner_NLP.png)

##  Ariadna Garmendia - Carrera de EspecializaciÃ³n en Inteligencia Artificial  <br /><br />

> ###   Contenido: <br />

*   â­ DesafÃ­o 1: [Word2vec](https://github.com/arigarmendia/NLP/blob/main/Desafio_1/Word2vector.ipynb)<br />
    ImplementaciÃ³n de varias funciones para transformar palabras a vectores, utilizando Ãºnicamente Numpy. Incluye identificaciÃ³n del vocabulario del corpus, transformaciÃ³n de los documentos del corpus con One-Hot encoding, TF, TF-IDF y una funciÃ³n para calcular Similitud Coseno.  

*   â­ DesafÃ­o 2: [Bot](https://github.com/arigarmendia/NLP/blob/main/Desafio_2/bot.ipynb) <br />
    ImplementaciÃ³n de un Bot simple en EspaÃ±ol con librerÃ­a Spacy Stanza. El bot busca la respuesta por medio del cÃ¡lculo de Similitud coseno comparando con todas las frases de un corpus previamente transformadas con TF-IDF. El texto del corpus fue tomado de una pÃ¡gina Web que contiene frases icÃ³nicas de los Simpsons. 

*   â­ DesafÃ­o 3: [Custom embeddings con Gensim](https://github.com/arigarmendia/NLP/tree/main/Desafio_3) <br />
    GeneraciÃ³n y ensayo de embeddings creados con la librerÃ­a Gensim a partir de los modelos CBOW y Skipgram. Los embeddings se generaron a partir de un corpus en espaÃ±ol basado en el cuento "El caballero de la armadura oxidada". Para visualizar agrupaciÃ³n entre vectores se utilizÃ³ TSNE con librerÃ­a Sklearn.

*   â­ DesafÃ­o 4: [Predictor de prÃ³xima palabra](https://github.com/arigarmendia/NLP/blob/main/Desafio_4/pred_next_word.ipynb) <br />
    Ensayo de predictor "many-to-one" utilizando embeddings y redes LSTM. Se utilizÃ³ un corpus basado en letras de canciones en inglÃ©s de la banda Coldplay (aprox. 62k palabras). Adicionalmente se probÃ³ un generador de secuencias autorregresivo utilizando los modelos ensayados.

*   â­ DesafÃ­o 5: [Sentiment analysis](https://github.com/arigarmendia/NLP/tree/main/Desafio_5)<br />
    Sentiment analysis utilizando el [dataset](https://www.kaggle.com/datasets/nicapotato/womens-ecommerce-clothing-reviews/code) de ecommerce clothing reviews de Kaggle. Se utilizaron custom embeddings y embedding pre-entrenados Fasttext con arquitectura de redes LSTM bidireccionales y se ejecutaron pruebas con y sin stop-words. Se implementaron tÃ©cnicas de undersampling y entrenamiento con pesos (class_weights) para contrarrestar los efectos del desbalance de clases en el dataset.


*   â­ DesafÃ­o 6: [bot_qa](https://github.com/arigarmendia/NLP/tree/main/Desafio_6)<br />
    Bot de preguntas y respuestas, conformado a partir de una arquitectura encoder-decoder con embeddings pre-entrenados Fasttext. Se analizaron 3 modelos basados en esencialmente la misma arquitectura pero con algunas variaciones (agregado de distintas opciones de dropout para mitigar el overfitting). <br /><br />

![](https://github.com/arigarmendia/NLP/blob/main/herramientas.png)<br /><br />

> ### ğŸ’¡ Estructura del repositorio:<br />
```bash
â”œâ”€â”€ Desafio_1
â”‚   â”œâ”€â”€ Word2vector.ipynb
â”œâ”€â”€ Desafio_2
â”‚   â”œâ”€â”€ bot.ipynb
â”œâ”€â”€ Desafio_3
â”‚   â”œâ”€â”€ my_embeddings.ipynb
â”‚   â”œâ”€â”€ graficos_TSNE
â”œâ”€â”€ Desafio_4
â”‚   â”œâ”€â”€ pred_next_word.ipynb
â”‚   â”œâ”€â”€ dataset
â”œâ”€â”€ Desafio_5
â”‚   â”œâ”€â”€ Desafio5_Notebook1.ipynb
â”‚   â”œâ”€â”€ Desafio5_Notebook2.ipynb
â”‚   â”œâ”€â”€ Pruebas_extra.ipynb
â”‚   â”œâ”€â”€ dataset
â”‚       â”œâ”€â”€ clothing_ecommerce_reviews.csv
â”œâ”€â”€ Desafio_6
â”‚   â”œâ”€â”€ bot_qa_model1.ipynb
â”‚   â”œâ”€â”€ bot_qa_model2.ipynb
â”‚   â”œâ”€â”€ bot_qa_model3.ipynb
â”‚   â”œâ”€â”€ conclusiones.md
â”œâ”€â”€ README.md
â”œâ”€â”€.gitignore
```
<br />

####    Preguntas? âœ‰ï¸ arigarmendia@gmail.com